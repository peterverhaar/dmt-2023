{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95b3a9f",
   "metadata": {},
   "source": [
    "Beste Peter, enorm bedankt voor je hulp met mijn eerste Python code! Ik hoop dat het een beetje helder is wat ik overal probeerde te doen, en tot vanmiddag :)\n",
    "\n",
    "In onderstaande wilde ik graag het woordaantal van het corpus berekenen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f21a61f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tdmh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtdmh\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_punctuation\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n\u001b[0;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompleteCorpus.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tdmh'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from tdmh import remove_punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "path = os.path.join('CompleteCorpus.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_text = file.read() \n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "print(len(words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce783637",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "In onderstaande zou ik graag de 100 meest frequente woorden zien. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a024ee65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tdmh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtdmh\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_punctuation\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompleteCorpus.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tdmh'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from tdmh import remove_punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "path = os.path.join('CompleteCorpus.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_text = file.read() \n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words) \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('illustration') \n",
    "stopwords.append('chapter')\n",
    "\n",
    "from collections import Counter \n",
    "freq = Counter(words) \n",
    "for word in freq.most_common(100): \n",
    "    if word not in stopwords:\n",
    "        print(word) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9523f",
   "metadata": {},
   "source": [
    "In onderstaande zou ik graag mijn lexicons inladen, maar ik begrijp de code niet 100 procent dus heb het er graag over of het zo goed zit, en hoe ik eventueel makkelijker alle lexicons tegelijk kan inladen. Ook lijkt het me mooi het grafisch te kunnen vormgeven, maar ik wist even niet hoe ik dat goed moest ombouwen. Ook zou ik graag eerst het corpus lemmatisen, maar ik wist niet zo goed hoe ik dat aan moest pakken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46c21dc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the corpus contains 1353 mentions; 0.047% of all tokens\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from nltk.tokenize import word_tokenize\n",
    "path = os.path.join('hunt.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_hunt_lexicon = file.read() \n",
    "hunt = full_hunt_lexicon.split()\n",
    "\n",
    "path = os.path.join('CompleteCorpus.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_text = file.read() \n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "tokens = len(words)\n",
    "        \n",
    "countOccurrences = 0\n",
    "for w in words:\n",
    "    if w in hunt:\n",
    "        countOccurrences += 1\n",
    "print( f'the corpus contains {countOccurrences} mentions; { round( (countOccurrences/tokens)*100, 3) }% of all tokens' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96351fd",
   "metadata": {},
   "source": [
    "Hieronder heb ik alle code herhaalt voor de andere 3 corpora. Waarschijnlijk voldoet dat niet aan de python-waarden om efficient te zijn maar het was even het enige wat ik kon bedenken :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3cb861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from nltk.tokenize import word_tokenize\n",
    "path = os.path.join('madness.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_madness_lexicon = file.read() \n",
    "madness = full_madness_lexicon.split()\n",
    "\n",
    "path = os.path.join('CompleteCorpus.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_text = file.read() \n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "tokens = len(words)\n",
    "        \n",
    "countOccurrences = 0\n",
    "for w in words:\n",
    "    if w in madness:\n",
    "        countOccurrences += 1\n",
    "print( f'the corpus contains {countOccurrences} mentions; { round( (countOccurrences/tokens)*100, 3) }% of all tokens' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3794c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from nltk.tokenize import word_tokenize\n",
    "path = os.path.join('love.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_love_lexicon = file.read() \n",
    "love = full_love_lexicon.split()\n",
    "\n",
    "path = os.path.join('CompleteCorpus.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_text = file.read() \n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "tokens = len(words)\n",
    "        \n",
    "countOccurrences = 0\n",
    "for w in words:\n",
    "    if w in love:\n",
    "        countOccurrences += 1\n",
    "print( f'the corpus contains {countOccurrences} mentions; { round( (countOccurrences/tokens)*100, 3) }% of all tokens' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456bba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from nltk.tokenize import word_tokenize\n",
    "path = os.path.join('magic.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_magic_lexicon = file.read() \n",
    "magic = full_magic_lexicon.split()\n",
    "\n",
    "path = os.path.join('CompleteCorpus.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_text = file.read() \n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "tokens = len(words)\n",
    "        \n",
    "countOccurrences = 0\n",
    "for w in words:\n",
    "    if w in magic:\n",
    "        countOccurrences += 1\n",
    "print( f'the corpus contains {countOccurrences} mentions; { round( (countOccurrences/tokens)*100, 3) }% of all tokens' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f57906",
   "metadata": {},
   "source": [
    "Tot slot zou ik graag de concordances zien van het complete magic lexicon. Ik wist niet hoe ik ze per zin kon concordancen zoals je met het corpus van het traineeship had gedaan, maar had nu snel het zo gedaan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84f6acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['magical', 'enchantment', 'spell', 'illusion', 'sorcery', 'wizard', 'mystical', 'magic', 'arcane', 'supernatural', 'witchcraft', 'enchant', 'magician', 'hex', 'magically', 'mystic', 'incantation', 'conjuration', 'conjure', 'mystique', 'wizardly', 'alchemy', 'wand', 'sorcerous', 'bewitch', 'wizardry', 'sorcerer', 'curse', 'spellcraft', 'potion', 'mysticism', 'supernaturalism', 'warlock', 'fairy', 'elf', 'enchantress', 'enchanter', 'conjurer', 'mage', 'conjury', 'summon', 'amulet', 'dragon', 'bewitching', 'occultism', 'abracadabra', 'witch', 'talisman', 'coven', 'sprite', 'conjuror', 'occult', 'superstition', 'hocuspocus', 'fortunetelling', 'divination', 'spellbinding', 'otherworldly', 'devilry', 'hoodoo', 'illusionist', 'grimoire', 'spellbind', 'charm', 'alakazam', 'spellworking', 'spellbound', 'pixie', 'spellwork', 'sorceress', 'soothsayer', 'elixir', 'arcanum', 'arcana', 'bewitchment']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from nltk.tokenize import word_tokenize\n",
    "path = os.path.join('magic.txt') \n",
    "file = open(path,encoding='utf-8') \n",
    "full_magic_lexicon = file.read() \n",
    "magic = full_magic_lexicon.split()\n",
    "print(magic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "812b695b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tdmh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize, word_tokenize\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtdmh\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompleteCorpus.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tdmh'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tdmh import *\n",
    "import os\n",
    "\n",
    "path = os.path.join('CompleteCorpus.txt')\n",
    "\n",
    "with open(path,encoding='utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "references = concordance_word( full_text , r'(magical)|(enchantment)|(spell)|(illusion)|(sorcery)|(wizard)|(mystical)|(magic)|(arcane)|(supernatural)|(witchcraft)|(enchant)|(magician)|(hex)|(magically)|(mystic)|(incantation)|(conjuration)|(conjure)|(mystique)|(wizardly)|(alchemy)|(wand)|(sorcerous)|(bewitch)|(wizardry)|(sorcerer)|(curse)|(spellcraft)|(potion)|(mysticism)|(supernaturalism)|(warlock)|(fairy)|(elf)|(enchantress)|(enchanter)|(conjurer)|(mage)|(conjury)|(summon)|(amulet)|(dragon)|(bewitching)|(occultism)|(abracadabra)|(witch)|(talisman)|(coven)|(sprite)|(conjuror)|(occult)|(superstition)|(hocuspocus)|(fortunetelling)|(divination)|(spellbinding)|(otherworldly)|(devilry)|(hoodoo)|(illusionist)|(grimoire)|(spellbind)|(charm)|(alakazam)|(spellworking)|(spellbound)|(pixie)|(spellwork)|(sorceress)|(soothsayer)|(elixir)|(arcanum)|(arcana)|(bewitchment)' , 20)\n",
    "\n",
    "\n",
    "number_of_results = 500\n",
    "\n",
    "print( f'Here are the first {number_of_results} occurrences:\\n\\n')\n",
    "for reference in references[:number_of_results]:\n",
    "    print( f'{reference}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751360b",
   "metadata": {},
   "source": [
    "Excuses dat de code op een aantal plekken een beetje rommelig is, ik hoop dat er nog een beetje uit is op te merken wat mijn intenties waren. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
